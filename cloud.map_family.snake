import os
import sys
import datetime

import pysam
from subprocess import CalledProcessError
from subprocess import check_output

import pandas as pd

import boto.dynamodb

from snakemake.remote.S3 import RemoteProvider as S3RemoteProvider

conn = boto.dynamodb.connect_to_region("us-east-1",
                                       aws_access_key_id=os.environ["AWS_ACCESS_KEY_ID"],
                                       aws_secret_access_key=os.environ["AWS_SECRET_ACCESS_KEY"])
S3 = S3RemoteProvider(access_key_id=os.environ["AWS_ACCESS_KEY_ID"], secret_access_key=os.environ["AWS_SECRET_ACCESS_KEY"])

INSTANCE_IP = check_output(["curl", "http://169.254.169.254/latest/meta-data/public-ipv4"]).decode("UTF-8")

SNAKEMAKE_DIR = os.path.dirname(workflow.snakefile)
WORKDIR = config.get("workdir", SNAKEMAKE_DIR)

shell.executable("/bin/bash")
shell.prefix("set -euo pipefail; ")

configfile: "cloud.config.yaml"

MANIFEST = config["manifest"]
REFERENCE = config["reference"]
MASKED_REF = config[REFERENCE]["masked_ref"]
CONTIGS_FILE = config[REFERENCE]["contigs"]

BAM_PARTITIONS = config["bam_partitions"]
UNMAPPED_PARTITIONS = config["unmapped_partitions"]
if UNMAPPED_PARTITIONS == -1:
    UNMAPPED_PARTITIONS = max(BAM_PARTITIONS // 500, 1)
MAX_BP = config["max_bp_in_mem"]

BUCKET = config["bucket"]
TMPDIR = config["tmpdir"]
LIVE_MERGE = config["live_merge"]
CLEAN_TEMP_FILES = config["clean_temp_files"]

if not os.path.exists("log"):
    os.makedirs("log")

CONTIGS = {}

with open(CONTIGS_FILE, "r") as reader:
    for line in reader:
        contig, size = line.rstrip().split()
        CONTIGS[contig] = int(size)

SAMPLES = pd.read_table(MANIFEST)
SAMPLE = config.get("sample", None)

if SAMPLE is not None:
    SAMPLES = SAMPLES.ix[SAMPLES.sn == SAMPLE,]

SAMPLES.index = SAMPLES.sn

def get_sparse_matrices_from_sample(wildcards):
    return ["%s/region_matrices/%s/%s.%d_%d" % (BUCKET, wildcards.sample, wildcards.sample, part, BAM_PARTITIONS) for part in range(BAM_PARTITIONS + UNMAPPED_PARTITIONS)]

localrules: all, get_headers, make_jobfile

rule all:
    input:  expand("%s/mapping/{sample}/{sample}/wssd_out_file" % BUCKET, sample = SAMPLES.sn)
            
rule merge_wssd:
    input: wssd = expand("%s/mapping/{{sample}}/{{sample}}/wssd_out_file.{contig}" % BUCKET, contig = CONTIGS)
    output: "%s/mapping/{sample}/{sample}/wssd_out_file" % BUCKET
    params: sge_opts = "-l mfree=8G -l data_scratch_ssd_disk_free=10G -pe serial 1 -N merge_sample -l h_rt=24:00:00"
    log: "log/merge_wssd/{sample}.txt"
    resources: mem=8
    benchmark: "benchmarks/wssd_merge/{sample}.txt"
    priority: 30
    run:
        shell('python3 merger.py {output} --infiles {input.wssd} --contigs_file {CONTIGS_FILE} --wssd_merge > {log} 2>&1')
        shell("aws s3 cp {output} s3://{output}")
        dynamo_table = conn.get_table("SimonsRDTracking")
        sample_item = dynamo_table.get_item(hash_key = wildcards.sample)
        sample_item["pytables_finished"] = 1
        sample_item.put()

rule merge_sparse_matrices:
    input: expand("%s/region_matrices/{{sample}}/{{sample}}.{part}_%d.h5" % (BUCKET, BAM_PARTITIONS), part = range(BAM_PARTITIONS + UNMAPPED_PARTITIONS))
    output: temp("%s/mapping/{sample}/{sample}/wssd_out_file.{contig}" % BUCKET)
    params: sge_opts = "-l mfree=8G -l data_scratch_ssd_disk_free=10G -pe serial 1 -N merge_sample -l h_rt=8:00:00"
    log: "log/merge/{sample}.{contig}.txt"
    resources: mem=8
    benchmark: "benchmarks/merger/{sample}.{contig}.txt"
    priority: 20
    run:
        infile_glob = os.path.commonprefix(get_sparse_matrices_from_sample(wildcards)) + "*"
        shell('python3 merger.py {output} --infile_glob "{infile_glob}" --contig {wildcards.contig} > {log} 2>&1')

rule map_and_count:
    input: bam = "bam/{sample}.bam",
           index = "bam/{sample}.bai"
    output: temp("%s/region_matrices/{sample}/{sample}.{part}_%d.h5" % (BUCKET, BAM_PARTITIONS))
    params: sge_opts = "-l mfree=5G -N map_count -l h_rt=5:00:00"
    benchmark: "benchmarks/counter/{sample}/{sample}.{part}.%d.txt" % BAM_PARTITIONS
    priority: 10
    resources: mem=4
    log: "%s/log/map/{sample}/{part}_%s.txt" % (WORKDIR, BAM_PARTITIONS)
    shadow: True
    run:
        masked_ref_name = os.path.basename(MASKED_REF)
        fifo = "mrsfast_fifo.%s.%s" %(wildcards.sample, str(wildcards.part))
        mrsfast_ref_path = MASKED_REF

        read_counter_args = "--max_basepairs_in_mem %d" % MAX_BP

        shell("hostname; echo part: {wildcards.part} nparts: {BAM_PARTITIONS} unmapped parts: {UNMAPPED_PARTITIONS} >> {log}; mkfifo {fifo}; ")
        shell("bin/bam_chunker_cascade -b {input.bam} -i {input.index} -p {wildcards.part} -n {BAM_PARTITIONS} -u {UNMAPPED_PARTITIONS} 2>> {log} | "
            "mrsfast --search {mrsfast_ref_path} -n 0 -e 2 --crop 36 --seq /dev/stdin -o {fifo} --disable-nohit >> {log} 2>> {log} | "
            "python3 read_counter.py {fifo} {output[0]} {CONTIGS_FILE} {read_counter_args} >> {log} 2>> {log}")
        shell("aws s3 cp {output[0]} s3://{output[0]}")
        dynamo_table = conn.get_table("SimonsRDTracking")
        sample_item = dynamo_table.get_item(hash_key = wildcards.sample, consistent_read=True)
        if "pytables_finished_chunks" not in sample_item:
            sample_item["pytables_finished_chunks"] = 0
	sample_item["pytables_finished_chunks"] += 1
        sample_item.put()

rule download_bam:
    output: temp("bam/{sample}.bam"), temp("bam/{sample}.bai")
    run:
        bam = SAMPLES.loc[wildcards.sample, "bam"]
        index = SAMPLES.loc[wildcards.sample, "index"]
        shell("aws s3 cp s3://{index} {output[1]}")
        shell("aws s3 cp s3://{bam} {output[0]}")
        dynamo_table = conn.get_table("SimonsRDTracking")
        sample_item = dynamo_table.get_item(hash_key = wildcards.sample)
        sample_item["instanceIP"] = INSTANCE_IP
        sample_item["last_download"] = datetime.datetime.now().strftime("%c")
        sample_item.put()
