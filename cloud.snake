import os
import sys
import pysam
import hashlib
import json
from MappingJob import *
from subprocess import CalledProcessError
import time

from snakemake.remote.S3 import RemoteProvider as S3RemoteProvider

S3 = S3RemoteProvider(access_key_id=os.environ["AWS_ACCESS_KEY_ID"], secret_access_key=os.environ["AWS_SECRET_ACCESS_KEY"])

if config == {}:
    configfile: "cloud.config.yaml"

MANIFEST = config["manifest"]
REFERENCE = config["reference"]
MASKED_REF = config[REFERENCE]["masked_ref"]
CONTIGS_FILE = config[REFERENCE]["contigs"]

BAM_PARTITIONS = config["bam_partitions"]
UNMAPPED_PARTITIONS = config["unmapped_partitions"]
AUTO_ASSIGN = config["auto_assign"]
MAX_BP = config["max_bp_in_mem"]

ARRAY_CONTIGS = config["array_contigs"]

AMAZON = config["amazon"]
BUCKET = config["bucket"]
TMPDIR = config["tmpdir"]
LIVE_MERGE = config["live_merge"]
CLEAN_TEMP_FILES = config["clean_temp_files"]

#files = S3.glob_wildcards("%s/{files}" % BUCKET)
#print(files)

if not AMAZON:
    shell.prefix("source config.sh; ")

if not os.path.exists("log"):
    os.makedirs("log")

CONTIGS = {}

with open(CONTIGS_FILE, "r") as reader:
    for line in reader:
        contig, size = line.rstrip().split()
        CONTIGS[contig] = int(size)

SAMPLES = {}

with open(MANIFEST, "r") as reader:
    for line in reader:
        sn, bam = line.rstrip().split()
        SAMPLES[sn] = bam

def get_map_jobs(wildcards):
    jobs = []
    print(wildcards.sample)
    finished = S3.glob_wildcards("%s/read_depth/%s/{files}" % (BUCKET, wildcards.sample))
    for part in range(BAM_PARTITIONS + UNMAPPED_PARTITIONS):
        job = "region_matrices/{0}/{0}.{1}_{2}.pkl".format(wildcards.sample, part, BAM_PARTITIONS)
        if job not in finished:
            jobs.append(S3.remote("%s/read_depth/%s/%s" % (BUCKET, wildcards.sample, job)))
    return jobs

def get_sparse_matrices_from_sample(wildcards):
    return ["region_matrices/%s/%s.%d_%d.pkl" % (wildcards.sample, wildcards.sample, part, BAM_PARTITIONS) for part in range(BAM_PARTITIONS + UNMAPPED_PARTITIONS)]

def get_multiple_contigs(sample, chr, num):
    names = SAMPLE_MAPPING_JOBS[sample]["%s.%s" % (chr, num)]
    return ["region_matrices/%s/%s.%s.pkl" % (sample, sample, region) for region in names]

localrules: all, get_headers, make_jobfile

rule all:
    input: expand("finished/{sample}.{ext}", sample = SAMPLES.keys(), ext = ["block", "txt"])

rule clean:
    input:  to_map = S3.remote(expand("%s/read_depth/{sample}/region_matrices/{sample}/{sample}.{part}_%d.pkl" % (BUCKET, BAM_PARTITIONS), sample = SAMPLES.keys(), part = range(BAM_PARTITIONS + UNMAPPED_PARTITIONS)))
    output: touch("finished/{sample}.txt")
    params: sge_opts = "-l h_rt=00:00:30"
    run:
        if CLEAN_TEMP_FILES:
            remove_glob = os.path.commonprefix(input.to_remove) + "*"
            shell("rm {remove_glob}")

rule map_and_count:
    input: lambda wildcards: "bams/%s.bam" % wildcards.sample, "bin/bam_chunker", "MRSFASTULTRA_INDEXED"
    output: S3.remote("%s/read_depth/{sample}/region_matrices/{sample}/{sample}.{part}_%d.pkl" % (BUCKET, BAM_PARTITIONS))
    params: sge_opts = "-l mfree=4G -N map_count -l h_rt=10:00:00"
    benchmark: "benchmarks/counter/{sample}/{sample}.{part}.%d.json" % BAM_PARTITIONS
    priority: 5
    resources: mem=4, cpu=1
    log: "log/map/{sample}/{part}_%s.txt" % BAM_PARTITIONS
    shadow: AMAZON
    run:
        masked_ref_name = os.path.basename(MASKED_REF)
        if AMAZON:
            fifo = "mrsfast_fifo"
            rsync_opts = ""
            mrsfast_ref_path = MASKED_REF
        else:
            fifo = "%s/mrsfast_fifo" % TMPDIR
            rsync_opts = "rsync {0}.index /var/tmp/mrsfast_index; touch /var/tmp/mrsfast_index/{0}; echo Finished rsync from {0} to /var/tmp/mrsfast_index > /dev/stderr; ".format(MASKED_REF)
            mrsfast_ref_path = "/var/tmp/%s" % masked_ref_name

        if ARRAY_CONTIGS != [] and ARRAY_CONTIGS is not None:
            common_contigs = "--common_contigs " + " ".join(ARRAY_CONTIGS)
        else:
            common_contigs = ""

        if AUTO_ASSIGN:
            read_counter_args = "--max_basepairs_in_mem %d" % MAX_BP
        else:
            read_counter_args = ""

        shell(
            "hostname; "
            "mkfifo {fifo}; "
            "{rsync_opts}"
            "./bin/bam_chunker -b {input[0]} -p {wildcards.part} -n {BAM_PARTITIONS} -u {UNMAPPED_PARTITIONS} | "
            "mrsfast --search /var/tmp/mrsfast_index/{masked_ref_name} -n 0 -e 2 --crop 36 --seq /dev/stdin -o {fifo} --disable-nohit >> /dev/stderr | "
            "python3 read_counter.py {fifo} {output} {CONTIGS_FILE} {common_contigs} {read_counter_args}"
            )
        if AMAZON:
            shell("aws s3 cp {output} s3://{BUCKET}/{output}")

rule block_download:
    input: "bams/{sample}.bam", "bams/{sample}.bam.bai"
    output: touch("finished/{sample}.block")
    params: wait = "finished/{sample}.txt"
    resources: disk=250
    priority: 100
    run:
        while not os.path.isfile(params.wait):
            time.sleep(60)

rule check_bam_files:
    input: "bam/{sample}.bam"
    output: touch("finished/{sample}.readable.txt")
    params: sge_opts = ""
    priority: 50
    run:
        for bamfile in input:
            try:
                test = pysam.AlignmentFile(bamfile)
            except ValueError as e:
                print("Error: could not open %s as bam.\n%s\n" % (bamfile, str(e)), file = sys.stderr)
                sys.exit(1)

rule download_sample:
    input: lambda wildcards: [S3.remote(SAMPLES[wildcards.sample]), S3.remote(SAMPLES[wildcards.sample] + ".bai")]
    output: temp("bams/{sample}.bam"), temp("bams/{sample}.bam.bai")
    resources: disk=250, cpu=1, mem=1
    priority: 10
    run:
        shell("cp {output[0]} ./")


rule get_headers:
    input: expand("bam_headers/{sample}.txt", sample = SAMPLES.keys())

rule get_header:
    input: lambda wildcards: SAMPLES[wildcards.sample]
    output: "bam_headers/{sample}.txt"
    params: sge_opts = ""
    shell:
        "samtools view -H {input} > {output}"

rule check_index:
    input: MASKED_REF
    output: touch("MRSFASTULTRA_INDEXED"), temp(".mrsfast_index_test_output.txt")
    params: sge_opts = ""
    run:
        try:
            shell("mrsfast --search {input[0]} --seq dummy.fq > {output[1]}")
        except CalledProcessError as e:
            sys.exit("Reference %s was not indexed with the current version of mrsfastULTRA. Please reindex." % input[0])

rule make_chunker:
    input: "src/chunker.cpp", "Makefile"
    output: "bin/bam_chunker"
    params: sge_opts = ""
    shell:
        "make"
