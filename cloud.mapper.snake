import os
import sys
import pysam
from subprocess import CalledProcessError

import pandas as pd

from snakemake.remote.S3 import RemoteProvider as S3RemoteProvider

S3 = S3RemoteProvider(access_key_id=os.environ["AWS_ACCESS_KEY_ID"], secret_access_key=os.environ["AWS_SECRET_ACCESS_KEY"])

SNAKEMAKE_DIR = os.path.dirname(workflow.snakefile)

shell.executable("/bin/bash")
shell.prefix("set -euo pipefail; ")

configfile: "cloud.config.yaml"

MANIFEST = config["manifest"]
REFERENCE = config["reference"]
MASKED_REF = config[REFERENCE]["masked_ref"]
CONTIGS_FILE = config[REFERENCE]["contigs"]

BAM_PARTITIONS = config["bam_partitions"]
UNMAPPED_PARTITIONS = config["unmapped_partitions"]
if UNMAPPED_PARTITIONS == -1:
    UNMAPPED_PARTITIONS = max(BAM_PARTITIONS // 500, 1)
MAX_BP = config["max_bp_in_mem"]

BUCKET = config["bucket"]
TMPDIR = config["tmpdir"]
LIVE_MERGE = config["live_merge"]
CLEAN_TEMP_FILES = config["clean_temp_files"]

if LIVE_MERGE:
    ruleorder: merge_sparse_matrices_live > merge_sparse_matrices
    ruleorder: merge_wssd > merge_sparse_matrices
else:
    ruleorder: merge_sparse_matrices > merge_sparse_matrices_live
    ruleorder: merge_sparse_matrices > merge_wssd

if not os.path.exists("log"):
    os.makedirs("log")

CONTIGS = {}

with open(CONTIGS_FILE, "r") as reader:
    for line in reader:
        contig, size = line.rstrip().split()
        CONTIGS[contig] = int(size)

SAMPLES = pd.read_table(MANIFEST)
# Only map given sample if specified
SAMPLE = config.get("sample", None)

if SAMPLE is not None:
    SAMPLES = SAMPLES.ix[SAMPLES.sn == SAMPLE,]

def get_sparse_matrices_from_sample(wildcards):
    return ["region_matrices/%s/%s.%d_%d.pkl" % (wildcards.sample, wildcards.sample, part, BAM_PARTITIONS) for part in range(BAM_PARTITIONS + UNMAPPED_PARTITIONS)]

def get_multiple_contigs(sample, chr, num):
    names = SAMPLE_MAPPING_JOBS[sample]["%s.%s" % (chr, num)]
    return ["region_matrices/%s/%s.%s.pkl" % (sample, sample, region) for region in names]

localrules: all, get_headers, make_jobfile

rule all:
    input:  S3.remote(expand("%s/mapping/{sample}/{sample}/wssd_out_file" % BUCKET, sample = SAMPLES.sn)),
            expand("finished/{sample}", sample = SAMPLES.sn)

rule clean:
    input:  to_remove = S3.remote(expand("%s/region_matrices/{{sample}}/{{sample}}.{part}_%d.{ext}" % (BUCKET, BAM_PARTITIONS), part = range(BAM_PARTITIONS + UNMAPPED_PARTITIONS), ext = ["dat", "bak", "dir"])), 
            to_keep = S3.remote("%s/mapping/{sample}/{sample}/wssd_out_file" % BUCKET)
    output: touch("finished/{sample}")
    params: sge_opts = "-l h_rt=01:00:00"
    run:
        if CLEAN_TEMP_FILES:
            remove_glob = os.path.commonprefix(input.to_remove) + "*"
            shell("rm {remove_glob}")

rule merge_sparse_matrices:
    input: S3.remote(expand("%s/region_matrices/{{sample}}/{{sample}}.{part}_%d.dat" % (BUCKET, BAM_PARTITIONS), part = range(BAM_PARTITIONS + UNMAPPED_PARTITIONS)))
    output: S3.remote("%s/mapping/{sample}/{sample}/wssd_out_file" % BUCKET)
    params: sge_opts = "-l mfree=8G -l data_scratch_ssd_disk_free=10G -pe serial 1 -N merge_sample -l h_rt=8:00:00"
    log: "log/merge/{sample}.txt"
    resources: mem=8
    benchmark: "benchmarks/merger/{sample}.txt"
    run:
        infile_glob = os.path.commonprefix(input) + "*"
        shell('python3 merger.py {output} --infile_glob "{infile_glob}" --per_contig_merge')

rule merge_wssd:
    input: S3.remote(expand("%s/mapping/{{sample}}/{{sample}}/wssd_out_file.{contig}" % BUCKET, contig = CONTIGS))
    output: S3.remote("%s/mapping/{sample}/{sample}/wssd_out_file" % BUCKET)
    params: sge_opts = "-l mfree=8G -l data_scratch_ssd_disk_free=10G -pe serial 1 -N merge_sample -l h_rt=24:00:00"
    resources: mem=8
    benchmark: "benchmarks/wssd_merge/{sample}.txt"
    priority: 30
    run:
        shell('python3 merger.py {output} --wssd_merge {input}')

rule merge_sparse_matrices_live:
    input: bam = "bam/{sample}.bam", chunker = "bin/bam_chunker_cascade", bam_check = "readable/{sample}.txt", index_check = "MRSFASTULTRA_INDEXED"
    output: S3.remote("%s/mapping/{sample}/{sample}/wssd_out_file.{contig}" % BUCKET)
    params: sge_opts = "-l mfree=8G -l data_scratch_ssd_disk_free=10G -pe serial 1 -N merge_sample -l h_rt=48:00:00"
    log: "log/merge/{sample}.{contig}.txt"
    resources: mem=8
    benchmark: "benchmarks/merger/{sample}.{contig}.txt"
    priority: 10
    run:
        infile_glob = os.path.commonprefix(get_sparse_matrices_from_sample(wildcards)) + "*"
        shell('python3 merger.py {output} --infile_glob "{infile_glob}" --live_merge --contig {wildcards.contig} >> {log} 2>> {log}')

rule map_and_count:
    input: bam = "bam/{sample}.bam", index = "bam/{sample}.bai", chunker = "bin/bam_chunker_cascade", readable = "readable/{sample}.txt", mrsfast_indexed = "MRSFASTULTRA_INDEXED"
    output: [S3.remote("%s/region_matrices/{sample}/{sample}.{part}_%d.%s") % (BUCKET, BAM_PARTITIONS, ext) for ext in ["dat", "dir"]], temp("%s/region_matrices/{sample}/{sample}.{part}_%d.%s" % (BUCKET, BAM_PARTITIONS, "bak"))
    params: sge_opts = "-l mfree=4G -N map_count -l h_rt=5:00:00"
    benchmark: "benchmarks/counter/{sample}/{sample}.{part}.%d.txt" % BAM_PARTITIONS
    priority: 20
    resources: mem=4
    log: "log/map/{sample}/{part}_%s.txt" % BAM_PARTITIONS
    shadow: True
    run:
        masked_ref_name = os.path.basename(MASKED_REF)
        ofprefix = output[0].replace(".dat", "")
        fifo = "mrsfast_fifo"
        mrsfast_ref_path = MASKED_REF

        read_counter_args = "--max_basepairs_in_mem %d" % MAX_BP

        shell("hostname; echo part: {wildcards.part} nparts: {BAM_PARTITIONS} unmapped parts: {UNMAPPED_PARTITIONS} >> {log}; mkfifo {fifo}; ")
        shell("{input.chunker} -b {input.bam} -i {input.index} -p {wildcards.part} -n {BAM_PARTITIONS} -u {UNMAPPED_PARTITIONS} 2>> {log} | "
            "mrsfast --search {mrsfast_ref_path} -n 0 -e 2 --crop 36 --seq /dev/stdin -o {fifo} --disable-nohit >> {log} 2>> {log} | "
            "python3 read_counter.py {fifo} {ofprefix} {CONTIGS_FILE} {read_counter_args} >> {log} 2>> {log}"
            )

rule check_bam_files:
    input: "bam/{sample}.bam"
    output: touch("readable/{sample}.txt")
    params: sge_opts = ""
    priority: 50
    run:
        for bamfile in input:
            try:
                test = pysam.AlignmentFile(bamfile)
            except ValueError as e:
                print("Error: could not open %s as bam.\n%s" % (bamfile, str(e)), file = sys.stderr)
                sys.exit(1)
