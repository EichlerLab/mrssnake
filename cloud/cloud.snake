import os
import sys
import pysam
import datetime
from subprocess import CalledProcessError
from subprocess import check_output

import boto3
import pandas as pd

SNAKEMAKE_DIR = os.path.dirname(workflow.snakefile)

shell.executable("/bin/bash")

path_string = "%s/../scripts" % SNAKEMAKE_DIR
if "PYTHONPATH" in os.environ:
    path_string = "$PYTHONPATH:%s" % path_string

shell.prefix("set -euo pipefail; export PYTHONPATH=%s; " % path_string)

configfile: "%s/cloud.config.yaml" % SNAKEMAKE_DIR

MANIFEST = config["manifest"]
REFERENCE = config["reference"]
MASKED_REF = config[REFERENCE]["masked_ref"]
CONTIGS_FILE = config[REFERENCE]["contigs"]

MAX_EDIST = config["max_edist"]

INSTANCE_IP = check_output(["curl", "http://169.254.169.254/latest/meta-data/public-ipv4"]).decode("UTF-8")
INSTANCE_ID = check_output(["curl", "http://169.254.169.254/latest/meta-data/instance-id"]).decode("UTF-8")

TMPDIR = config["tmpdir"]
CLEAN_TEMP_FILES = config["clean_temp_files"]

if not os.path.exists("log"):
    os.makedirs("log")

CONTIGS = {}

with open(CONTIGS_FILE, "r") as reader:
    for line in reader:
        contig, size = line.rstrip().split()
        CONTIGS[contig] = int(size)

SAMPLES=[config["sample"]]

dynamodb = boto3.resource('dynamodb',
                          region_name='us-east-1',
                          aws_access_key_id=os.environ["AWS_ACCESS_KEY_ID"],
                          aws_secret_access_key=os.environ["AWS_SECRET_ACCESS_KEY"]
                         )

def get_sparse_matrices_from_sample(wildcards):
    return ["region_matrices/%s/%s.%d_%d" % (wildcards.sample, wildcards.sample, part, BAM_PARTITIONS) for part in range(BAM_PARTITIONS + UNMAPPED_PARTITIONS)]

localrules: all, get_headers, make_jobfile, clean, make_chunker

rule all:
    input:  expand("uploaded/{sample}.txt", sample = SAMPLES)

rule upload_file:
    input: "mapping/{sample}/{sample}/wssd_out_file"
    output: touch("uploaded/{sample}.txt")
    params: upload_bucket=config["upload_bucket"]
    benchmark: "benchmarks/upload.{sample}.txt"
    run:
        shell("aws s3 cp {input} s3://{params.upload_bucket}/{wildcards.sample}.wssd.h5")
        table = dynamodb.Table(config["log_table"])
        entry = table.get_item(Key={'SimonsID': wildcards.sample})['Item']
        entry['Finished'] = 'True'
        entry['End'] = datetime.datetime.now().strftime("%c")
        table.put_item(Item=entry)

rule clean:
    input: "mapping/{sample}/{sample}/wssd_out_file"
    output: touch("finished/{sample}.txt")
    priority: 50
    run:
        if CLEAN_TEMP_FILES:
            shell("rm region_matrices/{wildcards.sample}/*")

if config["mode"] == "full":
    rule wssd_merge:
        input: wssd = expand("mapping/{{sample}}/{{sample}}/wssd_out_file.{contig}", contig = CONTIGS.keys()),
               shelve = expand("region_matrices/{{sample}}/{{sample}}.{part}_%d.h5" % BAM_PARTITIONS, part = range(BAM_PARTITIONS + UNMAPPED_PARTITIONS))
        output: "mapping/{sample}/{sample}/wssd_out_file"
        params: sge_opts="-l mfree=8G -l disk_free=10G -pe serial 1 -N merge_wssd -l h_rt=5:00:00 -soft -l gpfsstate=0"
        log: "log/wssd_merge/{sample}.txt"
        resources: mem=8
        priority: 40
        benchmark: "benchmarks/wssd_merge/{sample}.txt"
        run:
            tempfile = "%s/%s.wssd_out_file" % (TMPDIR, wildcards.sample)
            shell("python3 {SNAKEMAKE_DIR}/../scripts/merger.py {tempfile} --infiles {input.wssd} --wssd_merge --contigs_file {CONTIGS_FILE}")
            shell("rsync {tempfile} {output}")

    rule merge_sparse_matrices:
        input: expand("region_matrices/{{sample}}/{{sample}}.{part}_%d.h5" % BAM_PARTITIONS, part = range(BAM_PARTITIONS + UNMAPPED_PARTITIONS))
        output: temp("mapping/{sample}/{sample}/wssd_out_file.{contig}")
        params: sge_opts = "-l mfree=8G -l disk_free=10G -pe serial 1 -N merge_sample -l h_rt=5:00:00 -soft -l gpfsstate=0"
        log: "log/merge/{sample}.{contig}.txt"
        resources: mem=8
        priority: 30
        benchmark: "benchmarks/merge/{sample}.{contig}.txt"
        run:
            infile_glob = os.path.commonprefix(input) + "*"
            tempfile = "%s/%s.wssd_out_file.%s" % (TMPDIR, wildcards.sample, wildcards.contig)
            shell('python3 {SNAKEMAKE_DIR}/../scripts/merger.py {tempfile} --infile_glob "{infile_glob}" --contig {wildcards.contig}')
            shell("rsync {tempfile} {output}")

    rule map_and_count:
        input: bam = lambda wildcards: SAMPLES.ix[SAMPLES.sn == wildcards.sample, "path"], index = lambda wildcards: SAMPLES.ix[SAMPLES.sn == wildcards.sample, "index"]
        output: temp("region_matrices/{sample}/{sample}.{part}_%d.h5" % (BAM_PARTITIONS))
        params: sge_opts = "-l mfree=10G -N map_count -l h_rt=10:00:00 -soft -l gpfsstate=0"
        benchmark: "benchmarks/counter/{sample}/{sample}.{part}.%d.txt" % BAM_PARTITIONS
        resources: mem=10
        priority: 20
        log: "log/map/{sample}/{part}_%s.txt" % BAM_PARTITIONS
        run:
            masked_ref_name = os.path.basename(MASKED_REF)
            fifo = "%s/mrsfast_fifo" % TMPDIR
            if TMPDIR != "":
                local_index = "%s/%s" % (TMPDIR, os.path.basename(input.index[0]))
            else:
                local_index = input.index[0]
            mrsfast_ref_path = "/var/tmp/mrsfast_index/%s" % masked_ref_name
            rsync_opts = """rsync {0}.index /var/tmp/mrsfast_index/ --bwlimit 10000 --copy-links -p;
                            rsync {2} {3} --bwlimit 10000 --copy-links -p; 
                            if [[ ! -e {1} ]]; then touch {1}; fi; 
                            echo Finished rsync from {0} to {1} >> /dev/stderr; 
                            echo Finished rsync from {2} to {3} >> /dev/stderr; """.format(MASKED_REF, mrsfast_ref_path, input.index[0], local_index)

            read_counter_args = "--max_basepairs_in_mem %d --max_edist %s" % (MAX_BP, MAX_EDIST)
            shell("hostname; echo part: {wildcards.part} nparts: {BAM_PARTITIONS} unmapped parts: {UNMAPPED_PARTITIONS}; mkfifo {fifo}; {rsync_opts}")
            shell("{SNAKEMAKE_DIR}/../bin/bam_chunker -b {input.bam} -i {local_index} -p {wildcards.part} -n {BAM_PARTITIONS} -u {UNMAPPED_PARTITIONS} 2>> /dev/stderr | "
                "mrsfast --search {mrsfast_ref_path} -n 0 -e {MAX_EDIST} --crop 36 --seq /dev/stdin -o {fifo} --disable-nohit >> /dev/stderr | "
                "python3 {SNAKEMAKE_DIR}/../scripts/read_counter.py {fifo} {output} {CONTIGS_FILE} {read_counter_args}"
                )
elif config["mode"] == "fast":
    rule fast_count:
        input: "mapped/{sample}.raw.gz"
        output: touch("mapping/{sample}/{sample}/wssd_out_file")
        params: sge_opts="-l mfree=20G -l h_rt=24:00:00 -N map_{sample} -l disk_free=20G"
        benchmark: "benchmarks/counting.{sample}.txt"
        shell:
            """python3 {SNAKEMAKE_DIR}/../scripts/read_counter_from_gzfile.py {input} {output} --contigs_file {CONTIGS_FILE}"""

    rule fast_map:
        input: reads="data/{sample}.final.cram"
        output: touch("mapped/{sample}.raw.gz")
        params: sge_opts="-l mfree=4G -pe serial 4 -l h_rt=20:00:00 -N map_{sample}", input_ref=config["input_ref"]
        benchmark: "benchmarks/mapping.{sample}.txt"
        shell:
            """samtools view -T {params.input_ref} -h {input.reads} | \
               awk '(length($10)>=36 || substr($1,1,1)=="@") {{print}}' | \
               samtools fastq -nF 3840 - | \
               mrsfast --search {MASKED_REF} --crop 36 -n 0 -e 2 --seq /dev/stdin -o /dev/stdout \
                       --disable-nohit --threads 4 --mem 8 | 
               python {SNAKEMAKE_DIR}/../scripts/mrsfast_outputconverter.py | \
               gzip > {output}"""
else:
    raise ValueError("config['mode'] must be in ['fast', 'full'] but it is {}".format(config.get("mode", "not set")))

rule download_file:
    output: touch("data/{sample}.final.cram")
    params: download_bucket = config["download_bucket"]
    benchmark: "benchmarks/download.{sample}.txt"
    run:
        table = dynamodb.Table(config["log_table"])
        entry = table.get_item(Key={'SimonsID': wildcards.sample})['Item']
        entry['Finished'] = 'False'
        entry['Start'] = datetime.datetime.now().strftime("%c")
        entry['InstanceIP'] = INSTANCE_IP
        entry['InstanceID'] = INSTANCE_ID
        print(entry)
        table.put_item(Item=entry)
        shell("aws s3 cp --quiet --profile ssc s3://{params.download_bucket}/{wildcards.sample}.final.cram {output}")

