"""
This Snakefile downloads sample bams from the cloud 
and maps the sample by calling cloud.mapper.snake
"""

import os
import sys
import pysam
import hashlib
import json
from subprocess import CalledProcessError

import pandas as pd

SNAKEMAKE_DIR = os.path.dirname(workflow.snakefile)

from snakemake.remote.S3 import RemoteProvider as S3RemoteProvider

S3 = S3RemoteProvider(access_key_id=os.environ["AWS_ACCESS_KEY_ID"], secret_access_key=os.environ["AWS_SECRET_ACCESS_KEY"])

shell.executable("/bin/bash")
shell.prefix("source %s/config.sh; set -euo pipefail; " % SNAKEMAKE_DIR)

if config == {}:
    configfile: "cloud.config.yaml"

MANIFEST = config["manifest"]
REFERENCE = config["reference"]
MASKED_REF = config[REFERENCE]["masked_ref"]
CONTIGS_FILE = config[REFERENCE]["contigs"]

BAM_PARTITIONS = config["bam_partitions"]
UNMAPPED_PARTITIONS = config["unmapped_partitions"]
if UNMAPPED_PARTITIONS == -1:
    UNMAPPED_PARTITIONS = max(BAM_PARTITIONS // 500, 1)
AUTO_ASSIGN = config["auto_assign"]
MAX_BP = config["max_bp_in_mem"]

ARRAY_CONTIGS = config["array_contigs"]

AMAZON = config["amazon"]
TMPDIR = config["tmpdir"]
LIVE_MERGE = config["live_merge"]
CLEAN_TEMP_FILES = config["clean_temp_files"]

if not AMAZON:
    shell.prefix("source config.sh; ")

if not os.path.exists("log"):
    os.makedirs("log")

CONTIGS = {}

with open(CONTIGS_FILE, "r") as reader:
    for line in reader:
        contig, size = line.rstrip().split()
        CONTIGS[contig] = int(size)

SAMPLES = pd.read_table(MANIFEST)

def get_sparse_matrices_from_sample(wildcards):
    return ["region_matrices/%s/%s.%d_%d.pkl" % (wildcards.sample, wildcards.sample, part, BAM_PARTITIONS) for part in range(BAM_PARTITIONS + UNMAPPED_PARTITIONS)]

def get_multiple_contigs(sample, chr, num):
    names = SAMPLE_MAPPING_JOBS[sample]["%s.%s" % (chr, num)]
    return ["region_matrices/%s/%s.%s.pkl" % (sample, sample, region) for region in names]

localrules: all, get_headers, make_jobfile

rule all:
    input:  expand("mapping/{sample}/{sample}/wssd_out_file", sample = SAMPLES.sn),
            expand("finished/{sample}", sample = SAMPLES.sn)

rule map_sample:
    input: lambda wildcards: S3.remote(SAMPLES.ix[SAMPLES.sn == wildcards.sample, "bam"]),
           lambda wildcards: S3.remote(SAMPLES.ix[SAMPLES.sn == wildcards.sample, "index"])
    output: "mapping/{sample}/{sample}/wssd_out_file", "finished/{sample}"
    params: sge_opts = ""
    shell:
        "snakemake -s cloud.mapper.snake -j 40"

rule check_bam_files:
    input: [bam for bam in SAMPLES.bam]
    output: touch("BAMS_READABLE")
    params: sge_opts = ""
    priority: 50
    run:
        for bamfile in input:
            try:
                test = pysam.AlignmentFile(bamfile)
            except ValueError as e:
                print("Error: could not open %s as bam.\n%s" % (bamfile, str(e)), file = sys.stderr)
                sys.exit(1)

rule check_index:
    input: MASKED_REF
    output: touch("MRSFASTULTRA_INDEXED"), temp(".mrsfast_index_test_output.txt")
    params: sge_opts = ""
    run:
        try:
            shell("mrsfast --search {input[0]} --seq dummy.fq > {output[1]}")
        except CalledProcessError as e:
            sys.exit("Reference %s was not indexed with the current version of mrsfastULTRA. Please reindex." % input[0])

rule make_chunker:
    input: "src/chunker_cascade.cpp", "Makefile"
    output: "bin/bam_chunker_cascade"
    params: sge_opts = ""
    shell:
        "make"
