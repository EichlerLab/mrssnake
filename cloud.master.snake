"""
This Snakefile downloads sample bams from the cloud 
and maps the sample by calling cloud.mapper.snake
"""

import os
import sys
import pysam
from subprocess import CalledProcessError
from subprocess import check_output
import pandas as pd

import boto.dynamodb

SNAKEMAKE_DIR = os.path.dirname(workflow.snakefile)

from snakemake.remote.S3 import RemoteProvider as S3RemoteProvider

S3 = S3RemoteProvider(access_key_id=os.environ["AWS_ACCESS_KEY_ID"], secret_access_key=os.environ["AWS_SECRET_ACCESS_KEY"])
conn = boto.dynamodb.connect_to_region("us-east-1", 
                                       aws_access_key_id=os.environ["AWS_ACCESS_KEY_ID"], 
                                       aws_secret_access_key=os.environ["AWS_SECRET_ACCESS_KEY"])

INSTANCE_IP = check_output(["curl", "http://169.254.169.254/latest/meta-data/public-ipv4"]).decode("UTF-8")

shell.executable("/bin/bash")
shell.prefix("set -euo pipefail; ")

if config == {}:
    configfile: "cloud.config.yaml"

MANIFEST = config["manifest"]
REFERENCE = config["reference"]
MASKED_REF = config[REFERENCE]["masked_ref"]
CONTIGS_FILE = config[REFERENCE]["contigs"]

BAM_PARTITIONS = config["bam_partitions"]
UNMAPPED_PARTITIONS = config["unmapped_partitions"]
if UNMAPPED_PARTITIONS == -1:
    UNMAPPED_PARTITIONS = max(BAM_PARTITIONS // 500, 1)
MAX_BP = config["max_bp_in_mem"]

TMPDIR = config["tmpdir"]
LIVE_MERGE = config["live_merge"]
CLEAN_TEMP_FILES = config["clean_temp_files"]

BUCKET = config["bucket"]

if not os.path.exists("log"):
    os.makedirs("log")

CONTIGS = {}

with open(CONTIGS_FILE, "r") as reader:
    for line in reader:
        contig, size = line.rstrip().split()
        CONTIGS[contig] = int(size)

SAMPLES = pd.read_table(MANIFEST)

localrules: all

rule all:
    input:  S3.remote(expand("%s/mapping/{sample}/{sample}/wssd_out_file" % BUCKET, sample = SAMPLES.sn)), \
            expand("cleaned/{sample}.txt", sample = SAMPLES.sn)

rule clean:
    input: S3.remote("%s/mapping/{sample}/{sample}/wssd_out_file" % BUCKET)
    output: touch("cleaned/{sample}.txt")
    priority: 30
    run:
        dynamo_table = conn.get_table("SimonsRDTracking")
        sample_item = dynamo_table.get_item(hash_key = wildcards.sample)
        sample_item["finished"] = 1
        sample_item.put()
        shell("aws s3 rm s3://{BUCKET}/mapping/{wildcards.sample}/{wildcards.sample} --recursive --exclude wssd_out_file")
        shell("aws s3 rm s3://{BUCKET}/region_matrices/{wildcards.sample} --recursive")

rule map_sample:
    input: "bam/{sample}.bam", "bam/{sample}.bai"
    output: S3.remote("%s/mapping/{sample}/{sample}/wssd_out_file" % BUCKET)
    params: sge_opts = ""
    benchmark: "benchmarks/wssd_out/{sample}.txt"
    resources: cores=40, mem=156
    priority: 20
    shell:
        "snakemake -s cloud.mapper.snake --config sample={wildcards.sample} workdir={SNAKEMAKE_DIR} -j 30 --resources mem=130 -w 30 -T"

rule download_sample:
    input: lambda wildcards: S3.remote(SAMPLES.ix[SAMPLES.sn == wildcards.sample, "bam"]),
           lambda wildcards: S3.remote(SAMPLES.ix[SAMPLES.sn == wildcards.sample, "index"])
    output: temp("bam/{sample}.bam"),
            temp("bam/{sample}.bai")
    resources: cores=1
    priority: 10
    run:
        bam = SAMPLES.ix[SAMPLES.sn == wildcards.sample, "bam"].values[0]
        bai = SAMPLES.ix[SAMPLES.sn == wildcards.sample, "index"].values[0]
        shell("cp -p {bam} {output[0]}")
        shell("cp -p {bai} {output[1]}")
        dynamo_table = conn.get_table("SimonsRDTracking")
        sample_item = dynamo_table.get_item(hash_key = wildcards.sample)
        sample_item["instanceIP"] = INSTANCE_IP
        sample_item.put()
