"""
This Snakefile controls mapping jobs by calling local.mapper.snake
on a single sample. This prevents snakemake from sucking up too much memory
building the dag when mapping many samples.
"""

import os
import sys
import pysam
from subprocess import CalledProcessError

import pandas as pd

SNAKEMAKE_DIR = os.path.dirname(workflow.snakefile)

shell.executable("/bin/bash")
shell.prefix("set -euo pipefail; source %s/config.sh; " % SNAKEMAKE_DIR)

if config == {}:
    configfile: "globus.config.yaml"

MANIFEST = config["manifest"]
GLOBUS_MANIFEST = config["globus_manifest"]
GLOBUS_CONFIG = config["globus_config"]
GLOBUS_SETUP = config["globus_setup"]
SAMPLE_MAPPING = config["sample_mapping"]

REFERENCE = config["reference"]
MASKED_REF = config[REFERENCE]["masked_ref"]
CONTIGS_FILE = config[REFERENCE]["contigs"]

BAM_PARTITIONS = config["bam_partitions"]
UNMAPPED_PARTITIONS = config["unmapped_partitions"]
if UNMAPPED_PARTITIONS == -1:
    UNMAPPED_PARTITIONS = max(BAM_PARTITIONS // 500, 1)
MAX_BP = config["max_bp_in_mem"]

TMPDIR = config["tmpdir"]
LIVE_MERGE = config["live_merge"]
CLEAN_TEMP_FILES = config["clean_temp_files"]

NJOBS = str(config["njobs"])

if not os.path.exists("log"):
    os.makedirs("log")

GLOBUS_FILES = []

GLOBUS_DAT = pd.read_table(GLOBUS_MANIFEST)
GLOBUS_DAT.index = GLOBUS_DAT.wssdID

with open(GLOBUS_MANIFEST, "r") as globus:
    for line in globus:
        fn = line.rstrip()
        if fn.endswith(".bam") or fn.endswith(".bai"):
            GLOBUS_FILES.append(fn)

SAMPLE_DICT = {}

with open(SAMPLE_MAPPING, "r") as sn_map:
    for line in sn_map:
        fam, sn = line.rstrip().split()
        SAMPLE_DICT[fam] = sn

CONTIGS = {}

with open(CONTIGS_FILE, "r") as reader:
    for line in reader:
        contig, size = line.rstrip().split()
        CONTIGS[contig] = int(size)

SAMPLES = pd.read_table(MANIFEST)
if "sn" not in SAMPLES.columns:
    SAMPLES.sn = SAMPLES["Individual_ID"].str.replace(".", "_")
SAMPLES.index = SAMPLES.sn

localrules: all

rule all:
    input:  expand("mapping/{sample}/{sample}/wssd_out_file", sample = SAMPLES.sn)

#rule map_sample:
#    input: lambda wildcards: SAMPLES.loc[wildcards.sample, "bam"], lambda wildcards: SAMPLES.loc[wildcards.sample, "index"], "MRSFASTULTRA_INDEXED"
#    output: "mapping/{sample}/{sample}/wssd_out_file"
#    params: sge_opts = "-l mfree=8G -l h_rt=3:0:0:0 -N map.{sample}", tmpdir="tmp/{sample}"
#    benchmark: "benchmarks/wssd_out/{sample}.txt"
#    log: "%s/sample_map_log/{sample}.snakemake.txt" % SNAKEMAKE_DIR
#    priority: 20
#    run:
#        if not os.path.exists(params.tmpdir):
#            os.makedirs(params.tmpdir)
#        cwd = os.getcwd()
#        os.chdir(params.tmpdir)
#        shell("""~bnelsj/src/snakemake/bin/snakemake --drmaa " -V -cwd -e ./log -o ./log {{params.sge_opts}} -S /bin/bash" -s {SNAKEMAKE_DIR}/local.mapper.snake --config sample={wildcards.sample} workdir={SNAKEMAKE_DIR} -j {NJOBS} -w 30 &2> {log}""")
#        os.chdir(cwd)
#        shell("rsync {params.tmpdir}/{output} {output}")

rule clean:
    input: "mapping/{sample}/{sample}/wssd_out_file"
    output: "cleaned/{sample}.txt"
    params: sge_opts=""
    priority: 30
    shell:
        "rm bam/{sample}.*"

rule map_sample:
    input: "bam/{sample}.bam", "bam/{sample}.bai", "MRSFASTULTRA_INDEXED"
    output: "mapping/{sample}/{sample}/wssd_out_file"
    params: sge_opts = "-l mfree=8G -l h_rt=3:0:0:0 -N map.{sample}", tmpdir="tmp/{sample}"
    benchmark: "benchmarks/wssd_out/{sample}.txt"
    log: "%s/sample_map_log/{sample}.snakemake.txt" % SNAKEMAKE_DIR
    priority: 20
    shell:
        """
        mkdir -p {params.tmpdir};
        pushd {params.tmpdir};
        ~bnelsj/src/snakemake/bin/snakemake --drmaa " -V -cwd -w n -e ./log -o ./log {{params.sge_opts}} -S /bin/bash" \
        -s {SNAKEMAKE_DIR}/globus.mapper.snake --config sample={wildcards.sample} workdir={SNAKEMAKE_DIR} \
        -j {NJOBS} -w 30 > {log} 2>&1;
        popd;
        rsync {params.tmpdir}/{output} {output}
        """

rule download_all:
    input: expand("download_finished/{sample}.txt", sample = SAMPLES.sn)

rule download_sample:
    input: GLOBUS_MANIFEST 
    output: "bam/{sample}.bam", "bam/{sample}.bai", touch("download_finished/{sample}.txt")
    params: sge_opts = "-l mfree=2G -N dwnld_{sample} -l h_rt=1:0:0:0"
    log: "globus_log/{sample}.txt"
    run:
        import hashlib
        def hashfile(afile, hasher, blocksize=65536):
            buf = afile.read(blocksize)
            while len(buf) > 0:
                hasher.update(buf)
                buf = afile.read(blocksize)
            return hasher.hexdigest()

        fam_sn = wildcards.sample.replace("_", ".")
        sn = SAMPLE_DICT[fam_sn]
        try:
            globus_bam = GLOBUS_DAT.ix[wildcards.sample, "bam"]
            globus_bai = GLOBUS_DAT.ix[wildcards.sample, "bai"]
        except KeyError as e:
            print("Error: %s" % str(e), file=sys.stderr)
            sys.exit(1)
        bam_md5 = GLOBUS_DAT.ix[wildcards.sample, "md5_bam"]
        bai_md5 = GLOBUS_DAT.ix[wildcards.sample, "md5_bai"]
        bam_out = os.path.abspath(output[0])
        bai_out = os.path.abspath(output[1])
        download_bai_md5, download_bam_md5 = "", ""
        while bai_md5 != download_bai_md5:
            print("%s bai md5 remote: %s local: %s" % (wildcards.sample, bai_md5, download_bai_md5), flush=True)
            shell(""". ~/.globus/globus.config >> {log} 2>&1; . {GLOBUS_CONFIG} >> {log} 2>&1; bash {GLOBUS_SETUP} >> {log} 2>&1;
            globus-url-copy -vb -dbg -bs 200000000 -tcp-bs 200000000 -rst -rst-retries 0 gsiftp://fndca1.fnal.gov:2811//{globus_bai} file://{bai_out} >> {log} 2>&1""")
            download_bai_md5 = hashfile(open(bai_out, "rb"), hashlib.md5())
        while bam_md5 != download_bam_md5:
            print("%s bam md5 remote: %s local: %s" % (wildcards.sample, bam_md5, download_bam_md5), flush=True)
            shell(""". ~/.globus/globus.config >> {log} 2>&1; . {GLOBUS_CONFIG} >> {log} 2>&1; bash {GLOBUS_SETUP} >> {log} 2>&1;
            globus-url-copy -p 8 -vb -dbg -bs 200000000 -tcp-bs 200000000 -rst -rst-retries 0 gsiftp://fndca1.fnal.gov:2811//{globus_bam} file://{bam_out} >> {log} 2>&1""")
            download_bam_md5 = hashfile(open(bam_out, "rb"), hashlib.md5())

rule check_index:
    input: MASKED_REF
    output: touch("MRSFASTULTRA_INDEXED"), temp(".mrsfast_index_test_output.txt")
    params: sge_opts = ""
    run:
        try:
            shell("mrsfast --search {input[0]} --seq dummy.fq > {output[1]}")
        except CalledProcessError as e:
            sys.exit("Reference %s was not indexed with the current version of mrsfastULTRA. Please reindex." % input[0])

rule make_chunker:
    input: "src/chunker_cascade.cpp", "Makefile"
    output: "bin/bam_chunker_cascade"
    params: sge_opts = ""
    shell:
        "make"
